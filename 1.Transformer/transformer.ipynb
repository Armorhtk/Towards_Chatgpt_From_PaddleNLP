{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 前言\n",
    "\n",
    "学习飞桨官方精品课程，紧跟人工智能与AIGC趋势\n",
    "\n",
    "[零基础实践深度学习（第2版）](https://aistudio.baidu.com/aistudio/education/group/info/25302)\n",
    "\n",
    "[零门槛搞懂基于大模型的AIGC应用及技术要点](https://aistudio.baidu.com/aistudio/course/introduce/26723)\n",
    "\n",
    "目录：\n",
    "\n",
    "01 从Attention到Transformer\n",
    "\n",
    "02 通过Transformer实现机器翻译任务"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./image/transformer.png\" alt=\"图片来源飞桨零基础实践深度学习\" width=\"900\" height=\"600\" align=\"bottom\" />\n",
    "\n",
    "> 图片来源: 飞桨零基础实践深度学习（第二版）课程 7.2.3 Transformer模型\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2017年，由Vaswani等人在论文“[Attention Is All You Need](https://arxiv.org/abs/1706.03762)”中提出一种神经网络模型Transformer，其基于Seq2Seq网络结构进行建模，最开始被应用于机器翻译、语言建模和文本生成等自然语言处理任务上，其模型结构图如上图所示。\n",
    "\n",
    "与传统NLP特征提取类模型（CNN、RNN）相比，Transformer神经网络模型的主要区别在于：\n",
    "1）没有使用循环神经网络，纯基于注意力机制的结构搭建；\n",
    "2）引入了残差连接和层归一化；\n",
    "3）采用编码器-解码器结构来处理序列数据，并引入了位置编码，将位置信息嵌入到输入数据的向量表示中，让模型能够学习到序列中不同位置之间的相对位置关系。\n",
    "\n",
    "这些处理带来的优点有并行计算能力强、更好地捕捉上下文信息、更加灵活易扩展。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "class ScaledDotProductAttention(nn.Layer):\n",
    "    def __init__(self, dropout_p=0.):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(1-dropout_p)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None):\n",
    "        \"\"\"scaled dot product attention\"\"\"\n",
    "        # 计算scaling factor\n",
    "        embed_size = query.shape[-1]\n",
    "        scaling_factor = paddle.sqrt(paddle.to_tensor(embed_size,dtype='float32'))\n",
    "        # 注意力权重计算\n",
    "        # 计算query和key之间的点积，并除以scaling factor进行归一化\n",
    "        attn = paddle.matmul(query,key / scaling_factor)\n",
    "        # 注意力掩码机制\n",
    "        def masked_fill(x, mask, value):\n",
    "            y = paddle.full(x.shape, value, x.dtype)\n",
    "            return paddle.where(mask, y, x)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn = masked_fill(attn, attn_mask,-1e9)\n",
    "        # softmax，保证注意力权重范围在0-1之间\n",
    "        attn = self.softmax(attn)\n",
    "        # dropout\n",
    "        attn = self.dropout(attn)\n",
    "         # 对value进行加权\n",
    "        output = paddle.matmul(attn,value)\n",
    "        return (output,attn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k, pad_idx):\n",
    "    \"\"\"注意力掩码：识别序列中的<pad>占位符\n",
    "    Args:\n",
    "        seq_q (Tensor): query序列，shape = [batch size, query len]\n",
    "        seq_k (Tensor): key序列，shape = [batch size, key len]\n",
    "        pad_idx (Tensor): key序列<pad>占位符对应的数字索引\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.shape\n",
    "    batch_size, len_k = seq_k.shape\n",
    "    # 如果序列中元素对应<pad>占位符，则该位置在mask中对应元素为True\n",
    "    # pad_attn_mask: [batch size, key len]\n",
    "    pad_attn_mask = paddle.equal(seq_k, pad_idx)\n",
    "    # 增加额外的维度\n",
    "    # pad_attn_mask: [batch size, 1, key len]\n",
    "    pad_attn_mask = pad_attn_mask.unsqueeze(1)\n",
    "    # 将掩码广播到[batch size, query len, key len]\n",
    "    pad_attn_mask = paddle.broadcast_to(pad_attn_mask, (batch_size, len_q, len_k))\n",
    "\n",
    "    return pad_attn_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1, 6, 6], dtype=bool, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[[False, False, False, True , True , True ],\n",
      "         [False, False, False, True , True , True ],\n",
      "         [False, False, False, True , True , True ],\n",
      "         [False, False, False, True , True , True ],\n",
      "         [False, False, False, True , True , True ],\n",
      "         [False, False, False, True , True , True ]]])\n",
      "[1, 6] [1, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "# length = 6 , be like \"Hello World !\" --> [Hello,World,!,<pad>,<pad>,<pad>]\n",
    "q = k = paddle.to_tensor(np.array([[1, 1, 1, 0, 0, 0]]), dtype='float32')\n",
    "pad_idx = 0\n",
    "mask = get_attn_pad_mask(q, k, pad_idx)\n",
    "print(mask)\n",
    "print(q.shape, mask.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Self-Attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-Head Attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Layer):\n",
    "    def __init__(self, d_model, d_k, n_heads, dropout_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_O = nn.Linear(n_heads * d_k, d_model)\n",
    "        self.attention = ScaledDotProductAttention(dropout_p=dropout_p)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask):\n",
    "        \"\"\"\n",
    "        query: [batch_size, len_q, d_model]\n",
    "        key: [batch_size, len_k, d_model]\n",
    "        value: [batch_size, len_k, d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # 将query，key和value分别乘以对应的权重，并分割为不同的“头”\n",
    "        # q_s: [batch_size, len_q, n_heads, d_k]\n",
    "        # k_s: [batch_size, len_k, n_heads, d_k]\n",
    "        # v_s: [batch_size, len_k, n_heads, d_k]\n",
    "        q_s = self.W_Q(query).reshape([batch_size, -1, self.n_heads, self.d_k])\n",
    "        k_s = self.W_K(key).reshape([batch_size, -1, self.n_heads, self.d_k])\n",
    "        v_s = self.W_V(value).reshape([batch_size, -1, self.n_heads, self.d_k])\n",
    "\n",
    "        # 调整query，key和value的维度\n",
    "        # q_s: [batch_size, n_heads, len_q, d_k]\n",
    "        # k_s: [batch_size, n_heads, len_k, d_k]\n",
    "        # v_s: [batch_size, n_heads, len_k, d_k]\n",
    "        q_s = q_s.transpose((0, 2, 1, 3))\n",
    "        k_s = k_s.transpose((0, 2, 1, 3))\n",
    "        v_s = v_s.transpose((0, 2, 1, 3))\n",
    "\n",
    "        # attn_mask的dimension需与q_s, k_s, v_s对应\n",
    "        # attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        attn_mask = attn_mask.unsqueeze(1)\n",
    "        attn_mask = paddle.tile(attn_mask, (1, self.n_heads, 1, 1))\n",
    "\n",
    "        # 计算每个头的注意力分数\n",
    "        # context: [batch_size, n_heads, len_q, d_k]\n",
    "        # attn: [batch_size, n_heads, len_q, len_k]\n",
    "        context, attn = self.attention(q_s, k_s, v_s, attn_mask)\n",
    "\n",
    "        # concatenate\n",
    "        # context: [batch_size, len_q, n_heads * d_k]\n",
    "        context = context.transpose((0, 2, 1, 3)).reshape((batch_size, -1, self.n_heads * self.d_k))\n",
    "\n",
    "        # 乘以W_O\n",
    "        # output: [batch_size, len_q, n_heads * d_k]\n",
    "        output = self.W_O(context)\n",
    "\n",
    "        return output, attn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "Tensor(shape=[1, 2, 2], dtype=bool, place=Place(gpu:0), stop_gradient=True,\n       [[[False, False],\n         [False, False]]])"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddle.to_tensor(np.array([False])).broadcast_to((1, 2, 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 10] [1, 5, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "dmodel, dk, nheads = 10, 2, 5\n",
    "q = k = v =paddle.ones((1, 2, 10), dtype='float32')\n",
    "attn_mask = paddle.to_tensor(np.array([False])).broadcast_to((1, 2, 2))\n",
    "multi_head_attn = MultiHeadAttention(dmodel, dk, nheads)\n",
    "output, attn = multi_head_attn(q, k, v, attn_mask)\n",
    "print(output.shape, attn.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Positional Encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 通过Transformer实现机器翻译任务\n",
    "\n",
    "[PaddleNLP精选Example - 机器翻译](https://github.com/PaddlePaddle/PaddleNLP/tree/9e3bc459366faa04f70660e7881934dee1fa41b5/examples/machine_translation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 参考资料：\n",
    "\n",
    "[1] [Machine Translation using Transformer](https://github.com/PaddlePaddle/PaddleNLP/blob/21714c3797149f5283c9229313cb93fcb2c2d51a/examples/machine_translation/transformer/README.md)\n",
    "[2] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in Neural Information Processing Systems. 2017: 6000-6010.\n",
    "[3] Ba J L, Kiros J R, Hinton G E. Layer normalization[J]. arXiv preprint arXiv:1607.06450, 2016.\n",
    "[4] [PaddlePaddle文档](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/index_cn.html)\n",
    "[5] [零基础实践深度学习（第2版）](https://aistudio.baidu.com/aistudio/education/group/info/25302)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "作者：Armor\n",
    "邮箱：htkstudy163.com\n",
    "AI Studio主页：https://aistudio.baidu.com/aistudio/personalcenter/thirdview/392748"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}